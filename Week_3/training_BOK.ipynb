{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d79ab8",
   "metadata": {},
   "source": [
    "## [단계 ③] 학습, 테스트 데이터 선택\n",
    "③-1. 학습, 테스트 데이터 선택\n",
    "- 학습 데이터로는 길이가 적당한 정상적인 데이터로서 배터리 충전 데이터 중 하나인“1000_chg.csv”을 택한다.\n",
    "- 이상 감지 테스트를 위하여 불량이 있는 테스트 데이터를 선택하여 MTadGAN 테스트를 수행한다. 본 가이드북에서 사용될 테스트 데이터 파일은 “Test07_NG_dchg.csv”인데 이 데이터는 센서와이어 불량으로 인해 전압 이상을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fab98",
   "metadata": {},
   "source": [
    "## [단계 ④] AI모델 알고리즘 선택\n",
    "④-1. AI모델 알고리즘 선택\n",
    "- AI모델 알고리즘으로는 기존의 TadGAN 방법을 다변량 데이터로 확장한 MTadGAN 알고리즘을 주된 알고리즘으로 택하고 데이터의 차원을 축소하기 위하여 PCA 알고리즘을 초기 데이터 처리와 결합한다. MTadGAN 알고리즘을 사용하기 위해서는 Tensorflow 패키지를 설치하여야한다. \n",
    "- Tensorflow 및 분석에 필요한 패키지를 설치한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.5.0\n",
    "# !pip install graphviz==0.20.1\n",
    "# !pip install pydot==1.4.2\n",
    "# !pip install pydotplus==2.0.2\n",
    "# !pip install pyts==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab944e2",
   "metadata": {},
   "source": [
    "- 데이터에 대한 학습과 테스트에 필요한 라이브러리를 호출한다. 대표적인 라이브러리로는 tensorflow 이외에 데이터를 편리하게 다루기 위한 라이브러리로서 pandas, numpy가 있고 그래픽 도구로서는 plotly, matplotlib 등이 있으며 다양한 기계학습 기타 통계 계산 도구로서 sklearn(scikit-learn)이 있다.\n",
    "- pandas: 이차원의 데이터를 이름이 부여된 칼럼별로 테이블 형식으로 저장하고 편리하게 조작할 수 있도록 해준다.\n",
    "- numpy: 다차원의 배열로 이루어진 수치 데이터를 편리하게 다룰 수 있도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbf96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 라이브러리에서 필요한 모듈만 호출\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import math\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "import argparse\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Flatten, Dense, Reshape, UpSampling1D, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, Conv1D, LeakyReLU, Dropout, Add, Layer\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pydot\n",
    "import pydotplus\n",
    "from pydotplus import graphviz\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "from scipy.optimize import fmin\n",
    "from pyts.metrics import dtw\n",
    "from pandas.plotting import register_matplotlib_converters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2dff55",
   "metadata": {},
   "source": [
    "## [단계 ⑤] AI모델 학습\n",
    "⑤-1. AI모델 학습\n",
    "- 먼저 MTadGAN을 이용하기 위해 필요한 파라미터를 설정한다. 중요한 파라미터로는 win_size, features_dim이 있는데 win_size 파라미터는 시계열 데이터에서 MTadGAN 모형으로 입력하는 데이터의 시간 윈도우의 크기를 나타낸다. 본 가이드북에서는 win_size=10으로 설정한다. 한편 features_dim 파라미터는 MTadGAN 모형에서 받아들이는 데이터 피쳐의 차원수를 말하는데 여기서는 features_dim=3으로 설정한다. 따라서 원래 데이터의 개수(차원수)는 약 200 쯤 되므로 PCA 알고리즘을 써서 이를 3 차원으로 축소한다. 이에 더하여 Dictionary 자료형 변수로 params를 정의하여 여러 가지 파라미터를 정의하는데 예를 들면 params[‘epochs’] 은 MTadGAN 모형 학습(training)에서의 반복 횟수를 나타내고 params[‘learning_rate’]는 MTadGAN 모형 학습에서의 학습률을 나타내며 params[‘latent_dim’]는 입력 데이터 벡터를 부호화(encoding) 할 때의 숨은(잠재) 공간의 차원을 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7160d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win_size = 10, features_dim = 3 \n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "win_size =10                        # 시간의 윈도우 크기\n",
    "features_dim =3                     # PCA의 차원 수\n",
    "feat_dim = features_dim\n",
    "params = {}\n",
    "params['plot_network'] =True        # 네트워크 구조를 시각화할지 여부를 나타내는 플래그.\n",
    "params['epochs'] =30                # 학습 시 전체 데이터셋을 몇 번 반복할지를 나타내는 에폭 수.\n",
    "params['batch_size'] =64            # 한 번의 학습에 사용되는 데이터 샘플의 수를 나타내는 배치 크기\n",
    "params['n_critic'] =5               # 비판자(critic) 네트워크의 업데이트 횟수\n",
    "params['learning_rate'] =0.00005    # 모델 학습 시 가중치를 업데이트하는 학습률\n",
    "params['latent_dim'] =20            # 잠재 공간(latent space)의 차원 수. 일반적으로 생성 모델(GAN)에서 사용\n",
    "params['shape'] = [win_size, features_dim]                  # 입력 데이터의 기본 형태\n",
    "params['encoder_input_shape'] = [win_size, features_dim]    # 인코더 네트워크의 입력 형태\n",
    "params['encoder_reshape_shape'] = [20, 1]                   # 인코더 네트워크의 출력 형태를 변환할 때의 목표 형태\n",
    "params['generator_input_shape'] = [20, 1]                   # 생성기 네트워크의 입력 형태\n",
    "params['generator_reshape_shape'] = [win_size, 1]           # 생성기 네트워크의 출력 형태를 변환할 때의 목표 형태\n",
    "params['critic_x_input_shape'] = [win_size, features_dim]   # 비판자 네트워크의 X 입력 형태.\n",
    "params['critic_z_input_shape'] = [20, 1]                    # 비판자 네트워크의 Z 입력 형태\n",
    "print(\"win_size = %d, features_dim = %d \" % (win_size, features_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f348b8c",
   "metadata": {},
   "source": [
    "- 학습 데이터 파일 “1000_chg.csv”을 지정하여 args.signal_file이라는 변수를 저장하며 기타 변수를 정한다. 이상값의 파일이 있으면 args.anomaly_file이라는 변수에 저장한다. args.mode = ‘train’은 학습 모드임을 나타낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d770ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argments\n",
      "----------------------\n",
      "args.timest_form : 0\n",
      "args.signal_file : ../../../../data/preprocessed/train/1000_chg.csv\n",
      "args.anomaly_file : \n",
      "args.mode : train\n",
      "args.aggregate_interval : 1\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arguments=collections.namedtuple('Args',\n",
    "                            'signal_file timest_form anomaly_file mode aggregate_interval regate_interval')\n",
    "\n",
    "args=arguments(signal_file = '../../../../data/preprocessed/train/1000_chg.csv',\n",
    "    timest_form = 0,\n",
    "    anomaly_file = '',\n",
    "    mode = 'train',\n",
    "    aggregate_interval = 1,\n",
    "    regate_interval = 1)\n",
    "print(\"argments\")\n",
    "print(\"----------------------\")\n",
    "print(\"args.timest_form :\", args.timest_form)\n",
    "print(\"args.signal_file :\", args.signal_file)\n",
    "print(\"args.anomaly_file :\", args.anomaly_file)\n",
    "print(\"args.mode :\", args.mode)\n",
    "print(\"args.aggregate_interval :\", args.aggregate_interval)\n",
    "print(\"----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e508b",
   "metadata": {},
   "source": [
    "- 학습 데이터에 대한 차분 함수 diff_smooth_df()를 정의한다. 이 함수는 입력 데이터값을 일정한 시차(diffs_n>1)로 차이값을 구하거나 인접한 시점의 데이터 값들을 결합하여 데이터를 스무드하게 만드는 역할을 한다. 아래 코드에서는 diffs_n=0 등으로 지정하므로 차분함수가 원래 데이터를 그대로 두게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da9a3b",
   "metadata": {},
   "source": [
    "### 차분\n",
    "- 시계열 데이터의 분석 및 예측에서 중요한 기법 중 하나로, 데이터의 트렌드(추세)나 계절성을 제거하여 시계열 데이터를 안정화(stationary)시키는 데 사용\n",
    "- 차분은 시계열 데이터의 연속적인 데이터 포인트 간의 차이를 계산하는 것\n",
    "1. 1차 차분\n",
    "    - 연속적인 데이터 포인트 간의 차이를 계산\n",
    "    - 수식: $ \\Delta y_t = y_t - y_{t-1} $\n",
    "    - 주로 데이터의 단순 추세를 제거하는 데 사용\n",
    "2. 2차 차분\n",
    "    - 1차 차분을 한 번 더 차분\n",
    "    - 수식 : $ \\Delta^2 y_t = \\Delta y_t - \\Delta y_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2} $\n",
    "    - 더 복잡한 추세나 패턴을 제거하는 데 사용\n",
    "3. 계절 차분\n",
    "    - 특정 계절 주기를 고려하여 차분\n",
    "    - 수식 : $ \\Delta_s y_t = y_t - y_{t-s} $\n",
    "    - 주로 계절성을 제거하는 데 사용\n",
    "4. d차 차분\n",
    "    - 수식 : $ \\Delta^d y_t = \\sum_{k=0}^{d} (-1)^k \\binom{d}{k} y_{t-k} $\n",
    "#### 차분의 목적\n",
    "- 시계열 데이터의 비정상성을 제거해 데이터를 정상성을 띄게 만드는 것. 정상성을 띄는 시계열 데이터는 평균, 분산이 시간에 따라 변하지 않으며, 자기상관성이 일정한 특성을 가짐. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c9d6ab",
   "metadata": {},
   "source": [
    "### 평활화\n",
    "- 주로 이동 평균을 계산하는 방식으로 이루어짐. 평활화횟수는 이동 평균을 계산할 때 창의 크기를 나타냄. 예를 들어, 평활화 횟수가 3이면 각 데이터 포인트는 자신과 그 이전 2개 포인트를 포함한 3개의 데이터 포인트의 평균으로 대체.\n",
    "\n",
    "#### 효과\n",
    "- 노이즈 감소 : 평활화는 데이터의 단기적인 변동을 줄이고, 더 장기적인 추세를 강조\n",
    "- 추세 시각화 : 데이터의 주요 추세를 더 명확하게 볼 수 있게 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ccfa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffs_n:  0\n",
      "import diff_smooth_df() : Done ! \n"
     ]
    }
   ],
   "source": [
    "diffs_n = 0     # 차분을 적용할 횟수\n",
    "lags_n = 0      # 특징 벡터에 포함될 시차의 개수\n",
    "smooth_n = 0    # 특징 벡터에 포함될 최신 값들을 평활화할 횟수\n",
    "print(\"diffs_n: \", diffs_n)\n",
    "\n",
    "def diff_smooth_df(df, lags_n, diffs_n, smooth_n, diffs_abs=False, abs_features=False):\n",
    "    \"\"\"입력 데이터프레임에 대해 차분, 평활화, 시차를 적용하여 전처리\"\"\"\n",
    "    # 차분 차수가 1 이상인 경우 차분 적용\n",
    "    if diffs_n >= 1:\n",
    "        df = df.diff(diffs_n).dropna()\n",
    "    # 절댓값 차분을 적용\n",
    "    if diffs_abs == True:\n",
    "        df = abs(df)\n",
    "    # 이동 평균을 적용하여 데이터를 약간 평활화함\n",
    "    if smooth_n >= 2:\n",
    "        df = df.rolling(smooth_n).mean().dropna()\n",
    "    # 라그 값이 1 이상인 경우\n",
    "    if lags_n >= 1:\n",
    "        # 각 차원에 대해 차분 및 평활화된 값에 대해 lags_n 시차의 새로운 열을 추가함\n",
    "        df_columns_new = [f'{col}_lag{n}' for n in range(lags_n + 1) for col in df.columns]\n",
    "        df = pd.concat([df.shift(n) for n in range(lags_n + 1)], axis=1).dropna()\n",
    "        df.columns = df_columns_new\n",
    "    # 특징 벡터를 볼 때 명확성을 위해 열을 정렬함\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    # 모든 특징을 절대값으로 변환함 (만약 abs_features가 True인 경우)\n",
    "    if abs_features == True:\n",
    "        df = abs(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"import diff_smooth_df() : Done ! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8440280",
   "metadata": {},
   "source": [
    "- PCA 알고리즘을 적용하여 학습 데이터의 차원을 3차원으로 축소한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3166f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주성분 차원 축소 후 df.shape =  (6009, 4)\n",
      "----------------------\n",
      "   date      pca_1     pca_2     pca_3\n",
      "0     1  18.634349  0.564999  0.261410\n",
      "1     2  18.635771  0.571294  0.263656\n",
      "2     3  18.635892  0.570799  0.263567\n",
      "3     4  18.634366  0.564929  0.261397\n",
      "4     5  18.666694  0.565826  0.255594\n",
      "----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## 데이터 PCA 진행 ################################\n",
    "signal_path = args.signal_file\n",
    "df_train_0= pd.read_csv(signal_path)\n",
    "data_0 = df_train_0                 # 데이터 복사\n",
    "# preprocess or 'featurize' the training data\n",
    "data_1 = diff_smooth_df(data_0, lags_n, diffs_n, smooth_n)\n",
    "pca = PCA(n_components=features_dim)        # 입력 차원의 PCA 생성\n",
    "data = pca.fit_transform(data_1)            # 입력 데이터 PCA로 차원 축소 수행\n",
    "df_1 = []                                   # 데이터프레임 역할을 할 배열\n",
    "for i in range(len(data)):\n",
    "    row = [i+1]                             # 각 행의 인덱스 설정\n",
    "    for jj in range(features_dim):          # 차원 수 만큼 주성분 입력\n",
    "        row.append(data[i][jj])\n",
    "    df_1.append(row)                        # 데이터프레임에 추가\n",
    "df = pd.DataFrame(df_1)                     # 반복 수행 후 데이터 프레임화\n",
    "columns_new = ['date']                      # 데이터프레임의 인덱스열\n",
    "for i in range(1, features_dim+1):          # pca\n",
    "    pca_i = f'pca_{str(i)}'                 # 제i 주성분\n",
    "    columns_new.append(pca_i)\n",
    "df.columns = columns_new\n",
    "print(\"주성분 차원 축소 후 df.shape = \", df.shape)\n",
    "print(\"----------------------\")\n",
    "print(df.head(5))\n",
    "print(\"----------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9e8bf",
   "metadata": {},
   "source": [
    "- 입력 학습 데이터의 시간-집합화(time-aggregate) 과정을 수행하는 함수 time_segments_aggregate()를 정의한다. 이는 위에서 정의한 변수 args.aggregate_interval의 값이 1보다 클 때 그 구간 안의 데이터를 평균하거나하여 데이터를 조정하는 함수이다. 여기에서는 args.aggregate_interval=1로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24938be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
    "    \"\"\"주어진 시간 간격에 대해 값을 집계함.\n",
    "    Args:\n",
    "    X (ndarray 또는 pandas.DataFrame): N차원 시퀀스 값.\n",
    "    interval (int): 집계를 계산할 시간 간격을 나타내는 정수.\n",
    "    time_column (int): X에서 시간 값을 포함하는 열.\n",
    "    method (str 또는 list):\n",
    "    선택적. 집계 방법을 설명하는 문자열 또는 여러 집계 방법을 설명하는 문자열 목록. 지정하지 않으면 'mean'이 사용.\n",
    "    Returns:\n",
    "    ndarray, ndarray:\n",
    "    * 집계된 값의 시퀀스, 각 집계 방법에 대해 하나의 열.\n",
    "    * 집계된 각 세그먼트의 첫 번째 인덱스 값의 시퀀스.\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):   # 만약 X가 배열이라면 데이터프레임으로 변환\n",
    "        X = pd.DataFrame(X)\n",
    "    X = X.sort_values(time_column).set_index(time_column)   # 시간 열을 기준으로 정렬하고, 시간 열을 인덱스로 설정\n",
    "    if isinstance(method, str):     # 만약 `method`가 문자열이면, 리스트로 변환\n",
    "        method = [method]\n",
    "    start_ts = X.index.values[0]    # 데이터의 시작 시간\n",
    "    max_ts = X.index.values[-1]     # 데이터의 마지막 시간\n",
    "    values = list()                 # 집계된 값을 저장할 리스트\n",
    "    index = list()                  # 집계된 각 세그먼트의 시작 시간을 저장할 리스트\n",
    "    while start_ts <= max_ts:       # 시작부터 마지막까지\n",
    "        end_ts = start_ts + interval    # 시간 간격을 계산\n",
    "        subset = X.loc[start_ts:end_ts-1]   # 해당 간격을 선택\n",
    "        aggregated = [\n",
    "            getattr(subset, agg)(skipna=True).values\n",
    "            for agg in method]\n",
    "        values.append(np.concatenate(aggregated))   # 집계 결과 추가\n",
    "        index.append(start_ts)                      # 시작 시간을 리스트에 추가\n",
    "        start_ts = end_ts                           # 시작 시간 갱신(현재 간격 종료 시간으로)\n",
    "    return np.asarray(values), np.asarray(index)    # 집계 결과를 ndarray로 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1676c03",
   "metadata": {},
   "source": [
    "- time_segments_aggregate() 함수를 작용하고 이들 값을 [-1,+1] 사이의 값으로 정규화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9b4c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal data (after time_segments_aggregate) =  (6009, 3)\n",
      "--------------------------------------------\n",
      "[[18.6343489   0.56499908  0.26141038]\n",
      " [18.63577092  0.57129376  0.26365645]\n",
      " [18.63589171  0.57079932  0.26356652]\n",
      " [18.63436617  0.56492876  0.26139721]\n",
      " [18.66669403  0.56582562  0.25559404]]\n",
      "--------------------------------------------\n",
      "\n",
      "\n",
      "X (after MinMaxScaler) =  (6009, 3)\n",
      "--------------------------------------------\n",
      "[[0.99721634 0.48867576 0.26892606]\n",
      " [0.99732535 0.49487412 0.27233804]\n",
      " [0.99733461 0.49438724 0.27220142]\n",
      " [0.99721766 0.48860651 0.26890604]\n",
      " [0.99969584 0.48948965 0.2600905 ]]\n",
      "--------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 시간 집합화 수행\n",
    "X, index = time_segments_aggregate(df, interval=args.aggregate_interval, time_column='date')\n",
    "print(\"signal data (after time_segments_aggregate) = \", X.shape)\n",
    "print(\"--------------------------------------------\")\n",
    "print(X[:5])\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 최대최소 정규화 적용\n",
    "X = SimpleImputer().fit_transform(X)\n",
    "X = MinMaxScaler(feature_range=(-1, 1)).fit_transform(X)\n",
    "print(\"X (after MinMaxScaler) = \", X.shape)\n",
    "print(\"--------------------------------------------\")\n",
    "print(X[:5])\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "X_norm = X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735aa9b4",
   "metadata": {},
   "source": [
    "- 학습 데이터를 AI 알고리즘이 요구하는 입력 윈도우 길이(win_size)에 맞게 묶는 함수인 rolling_window_sequences() 함수를 정의한다. 즉, 만일 win_size=10이면 연속한 10개의 데이터 값을 벡터로 묶어서 입력값으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00bb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_sequences(X, index, window_size, target_size, step_size,\n",
    "    target_column, drop=None, drop_windows=False):\n",
    "    \"\"\"시계열 데이터에서 롤링 윈도우 시퀀스를 생성함.\n",
    "    이 함수는 입력 시퀀스를 롤링 윈도우로 순회하며 입력 시퀀스 배열과 목표 시퀀스 배열을 생성함.\n",
    "    선택적으로 시퀀스에서 특정 값을 삭제할 수 있음.\n",
    "    \n",
    "    Args:\n",
    "    X (ndarray): 순회할 N차원 시퀀스.\n",
    "    index (ndarray): X의 인덱스 값을 포함하는 배열.\n",
    "    window_size (int): 입력 시퀀스의 길이.\n",
    "    target_size (int): 목표 시퀀스의 길이.\n",
    "    step_size (int):  # 윈도우를 각 라운드마다 몇 단계씩 앞으로 이동할지 나타내는 값.\n",
    "    target_column (int): X의 어느 열이 목표인지를 나타내는 값.\n",
    "    drop (ndarray 또는 None 또는 str 또는 float 또는 bool):  # 선택적. X의 어떤 값이 유효하지 않은지 나타내는 부울 값 배열 또는 삭제할 값을 나타내는 값. 지정하지 않으면 `None`이 사용됨.\n",
    "    drop_windows (bool): 선택적. 삭제 기능을 활성화할지 여부를 나타냄. 지정하지 않으면 `False`가 사용됨.\n",
    "    \n",
    "    Returns:\n",
    "    ndarray, ndarray, ndarray, ndarray:\n",
    "    * 입력 시퀀스.\n",
    "    * 목표 시퀀스.\n",
    "    * 각 입력 시퀀스의 첫 번째 인덱스 값.\n",
    "    * 각 목표 시퀀스의 첫 번째 인덱스 값.\n",
    "    \"\"\"\n",
    "    out_X = list()\n",
    "    out_y = list()\n",
    "    X_index = list()\n",
    "    y_index = list()\n",
    "    target = X[:, target_column]\n",
    "    if drop_windows:\n",
    "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
    "            if len(drop) != len(X):\n",
    "                raise Exception('배열 `drop`과 `X`는 같은 길이여야 합니다.')\n",
    "        else:\n",
    "            if isinstance(drop, float) and np.isnan(drop):\n",
    "                drop = np.isnan(X)\n",
    "            else:\n",
    "                drop = X == drop\n",
    "    start = 0\n",
    "    max_start = len(X) - window_size - target_size + 1\n",
    "    while start < max_start:\n",
    "        end = start + window_size\n",
    "        if drop_windows:\n",
    "            drop_window = drop[start:end + target_size]\n",
    "            to_drop = np.where(drop_window)[0]\n",
    "            if to_drop.size:\n",
    "                start += to_drop[-1] + 1\n",
    "                continue\n",
    "        out_X.append(X[start:end])\n",
    "        out_y.append(target[end:end + target_size])\n",
    "        X_index.append(index[start])\n",
    "        y_index.append(index[end])\n",
    "        start = start + step_size\n",
    "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2565761",
   "metadata": {},
   "source": [
    "- 학습 데이터에 rolling_window_sequences() 함수를 적용하여 MTadGAN 알고리즘에 입력할 수 있도록 만든다. 즉, MTadGAN에서는 시계열 데이터를 모형으로 입력할 때에 특정한 시점에 대하여 일정한 윈도우 길이(win_size)만큼 묶어서 입력하게 된다. rolling_window_sequences() 함수는 입력 윈도우 길이(win_size)에 맞게 모든 시점에 대하여 묶는 함수이다. 만일 win_size=10이면 모든 시점에 대하여 각각 근처의 연속한 10개의 데이터 값을 벡터로 묶어서 입력값으로 사용할 수 있게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fb9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (after rolling_window_seq): (5999, 10, 3)\n",
      "X : \n",
      "[[[0.99721634 0.48867576 0.26892606]\n",
      "  [0.99732535 0.49487412 0.27233804]\n",
      "  [0.99733461 0.49438724 0.27220142]\n",
      "  [0.99721766 0.48860651 0.26890604]\n",
      "  [0.99969584 0.48948965 0.2600905 ]]\n",
      "\n",
      " [[0.99732535 0.49487412 0.27233804]\n",
      "  [0.99733461 0.49438724 0.27220142]\n",
      "  [0.99721766 0.48860651 0.26890604]\n",
      "  [0.99969584 0.48948965 0.2600905 ]\n",
      "  [0.99729103 0.49165725 0.27192521]]\n",
      "\n",
      " [[0.99733461 0.49438724 0.27220142]\n",
      "  [0.99721766 0.48860651 0.26890604]\n",
      "  [0.99969584 0.48948965 0.2600905 ]\n",
      "  [0.99729103 0.49165725 0.27192521]\n",
      "  [0.99741076 0.49573991 0.26903184]]]\n",
      "X index shape: (5999,)\n",
      "y shape: (5999, 1)\n",
      "y : \n",
      "[[0.99860594]\n",
      " [0.99594234]\n",
      " [0.99452801]\n",
      " [1.        ]\n",
      " [0.9959476 ]]\n",
      "y index shape: (5999,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 윈도우 길이에 맞게 묵음\n",
    "X, y, X_index, y_index=rolling_window_sequences(X, index, window_size=win_size, target_size=1, step_size=1, target_column=0)\n",
    "print(\"X shape (after rolling_window_seq): {}\".format(X.shape))\n",
    "print(\"X : \")\n",
    "print(X[:3, :5])\n",
    "print(\"X index shape: {}\".format(X_index.shape))\n",
    "print(\"y shape: {}\".format(y.shape))\n",
    "print(\"y : \")\n",
    "print(y[:5])\n",
    "print(\"y index shape: {}\".format(y_index.shape))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40197a3",
   "metadata": {},
   "source": [
    "- GPU가 있는지 체크한다. GPU가 있을 시 GPU 정보가 나오며, GPU가 없을 시 아래와 같은 결과가 나오게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5128f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# GPU 환경 존재 여부 확인\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "print (gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03108bfa",
   "metadata": {},
   "source": [
    "- MTadGAN에 필요한 함수 중 하나로서 RandomWeightedAverage() 클래스를 정의하는데 이는 입력 데이터와 예측된 입력 데이터를 정해진 비율로 선형 결합하는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e62c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomWeightedAverage() Class defined, Done! \n"
     ]
    }
   ],
   "source": [
    "class RandomWeightedAverage(Layer):\n",
    "    def __init__(self, batch_size):\n",
    "        \"\"\"레이어 초기화\n",
    "        Args:\n",
    "        batch_size: 배치 크기 (예: 64)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"랜덤 가중 평균 계산\n",
    "        Args:\n",
    "        inputs[0] x: 원래 입력\n",
    "        inputs[1] x_: 예측된 입력\n",
    "        \"\"\"\n",
    "        alpha = K.random_uniform((self.batch_size, 1, 1))  # 배치 크기에 따라 랜덤 가중치 생성\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])  # 원래 입력과 예측된 입력의 랜덤 가중 평균을 반환\n",
    "\n",
    "print(\"RandomWeightedAverage() Class defined, Done! \") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d31b6",
   "metadata": {},
   "source": [
    "- MTadGAN의 encoder 레이어를 생성하는 함수를 정의한다. 이는 입력 데이터를 숨은 공간으로 임베딩하는 기능을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "336a714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 레이어 생성 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def build_encoder_layer(input_shape, encoder_reshape_shape):\n",
    "    \"\"\"인코더 레이어를 생성\n",
    "    인자들:\n",
    "    input_shape: [10, 1]\n",
    "    encoder_reshape_shape: [20, 1]\n",
    "    \n",
    "    Returns:\n",
    "    인코더 모델\n",
    "    \"\"\"\n",
    "    x = Input(shape=input_shape)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Bidirectional(LSTM(units=win_size, return_sequences=True)), \n",
    "        Flatten(),\n",
    "        Dense(20), # 20 = self.critic_z_input_shape[0]\n",
    "        Reshape(target_shape=encoder_reshape_shape)]) # (20, 1)\n",
    "    return Model(x, model(x))\n",
    "\n",
    "print(\"인코더 레이어 생성 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4cf6b",
   "metadata": {},
   "source": [
    "- Bidirectional : LSTM 레이어를 감싸서 순방향과 역방향 모두 입력 시퀀스를 처리하도록 함\n",
    "    - units : LSTM 레이어의 유닛(뉴런) 수. win_size 만큼의 unit을 가짐\n",
    "    - return_sequences = True : 각 타임스텝마다 생성된 출력 벡터 반환\n",
    "    - units 인자를 증가시키는 경우 더 복잡한 학습이 가능하지만 과적합의 위험이 커짐\n",
    "- Flatten : 다차원 텐서 배열을 차원으로 변환시키는 역할\n",
    "- Dense : 완전 연결층. 모든 입력 뉴런이 모든 출력 뉴런과 연결되도록 함.\n",
    "    - units : 뉴런의 수. 여기서 \n",
    "    - activation : 활성화 함수. 미지정 시 linear(선형)\n",
    "- Reshape : 입력 데이터의 차원을 변경하는 역할\n",
    "- 모델의 층\n",
    "```\n",
    "Input Layer: (10, 1)\n",
    "    |\n",
    "Bidirectional LSTM Layer (units=win_size): (10, 2 * win_size)\n",
    "    |\n",
    "Flatten Layer: (10 * 2 * win_size)\n",
    "    |\n",
    "Dense Layer (units=20): (20)\n",
    "    |\n",
    "Reshape Layer (target_shape=(20, 1)): (20, 1) : 출력층의 역할\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc03760",
   "metadata": {},
   "source": [
    "- MTadGAN의 generator 레이어를 생성하는 함수를 정의한다. 이는 숨은 공간으로부터 가상의 데이터를 생성하는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95db75bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성기 레이어 생성 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "def build_generator_layer(input_shape, generator_reshape_shape):\n",
    "    # input_shape = (20, 1) / generator_reshape_shape = (50, 1)\n",
    "    x = Input(shape=input_shape)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Flatten(),  # 입력을 1차원으로 평탄화\n",
    "        Dense(win_size),  # 원래 50이었던 win_size만큼의 유닛을 가진 Dense 레이어\n",
    "        Reshape(target_shape=generator_reshape_shape),  # (50, 1) 형태로 변환\n",
    "        # 양방향 LSTM 레이어, 64 유닛, 시퀀스를 반환하며, 순방향과 역방향 출력을 연결\n",
    "        Bidirectional(LSTM(units=64, return_sequences=True), merge_mode='concat'),  \n",
    "        Dropout(rate=0.2),  # 드롭아웃 20%\n",
    "        #UpSampling1D(size=2),\n",
    "        UpSampling1D(size=1),  # 시퀀스 데이터를 1배 업샘플링\n",
    "        # 양방향 LSTM 레이어, 64 유닛, 시퀀스를 반환하며, 순방향과 역방향 출력을 연결\n",
    "        Bidirectional(LSTM(units=64, return_sequences=True), merge_mode='concat'),  \n",
    "        Dropout(rate=0.2),  # 드롭아웃 20%\n",
    "        # 시퀀스 데이터의 각 타임 스텝에 대해 Dense 레이어를 적용. features_dim은 출력 특성의 수(PCA 차원 수)\n",
    "        TimeDistributed(Dense(features_dim)),  \n",
    "        Activation(activation='tanh')])  # tanh 활성화 함수를 사용하여 출력. 최종 출력 형태는 (None, 10, 1)\n",
    "    return Model(x, model(x))\n",
    "print(\"생성기 레이어 생성 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00d961",
   "metadata": {},
   "source": [
    "- Bidrectional\n",
    "    - merge_mode : 두 방향의 출력을 어떻게 결합할지\n",
    "        - sum : 순방향과 역방향 출력을 더함\n",
    "        - mul : 곱함\n",
    "        - ave : 평균\n",
    "        - concat : 연결하여 결합. 차원 수 2배\n",
    "- upSampling1D : 업샘플링하여 시퀀스의 길이를 늘림\n",
    "    - size = 1 : 시퀀스의 길이는 변하지 않음\n",
    "    - size = 2 : 시퀀스의 타임 스텝이 2배가 됨\n",
    "- Dropout : 일부 뉴런을 학습에서 제외하여 과적합을 방지하는 역할\n",
    "    - rate : 드롭아웃 비율\n",
    "- TimeDistributed : 입력 시퀀스의 각 타임 스텝에 대해 지정된 레이어를 독립적으로 적용\n",
    "- Activation : tanh\n",
    "    - 신경망의 출력을 특정 범위로 변환하는 데 사용. \n",
    "    - 신경망의 각 뉴런이 출력하는 값을 변환해 비선형성을 도입. 모델이 더 복잡한 패턴을 학습하도록 함\n",
    "    - tanh 활성화 함수는 출력을 -1과 1 사이로 조절(주성분 분석을 위한 출력 결과)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d164860",
   "metadata": {},
   "source": [
    "- MTadGAN의 판별자 critic_x 레이어를 만드는 함수를 정의한다. 여기에서 critic_x는 실제의 학습 데이터와 숨은 공간에서 생성된 가상의 데이터를 구별해내는 판별자 네트워크를 말한다. 여기에서 critic_x는 실제의 학습 데이터와 숨은 공간에서 생성된 가상의 데이터를 구별해내는 판별자 네트워크를 말한다. 즉, 실제의 데이터 X 에서 나오는 시계열 시퀀스들과 생성자 G(z) 에서 생성된 가짜의 시계열 시퀀스들을 분별해내는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e56a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_x 레이어 생성 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# win_size에 따라 커널 사이즈 조정\n",
    "if win_size >= 30:\n",
    "    k_size = 5\n",
    "else:\n",
    "    k_size = 2\n",
    "\n",
    "def build_critic_x_layer(input_shape):\n",
    "    \"\"\"critic_x 레이어 생성\n",
    "    Args:\n",
    "    critic_x_input_shape: [10, 1]\n",
    "    Returns:\n",
    "    critic_x 모델\n",
    "    \"\"\"\n",
    "    x = Input(shape=input_shape)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Conv1D(filters=64, kernel_size=k_size),  # 64개 필터, \n",
    "        LeakyReLU(alpha=0.2),  # LeakyReLU 활성화 함수, \n",
    "        Dropout(rate=0.25),  # 드롭아웃 25%\n",
    "        Conv1D(filters=64, kernel_size=k_size),  \n",
    "        LeakyReLU(alpha=0.2),  \n",
    "        Dropout(rate=0.25),  \n",
    "        Conv1D(filters=64, kernel_size=k_size),  \n",
    "        LeakyReLU(alpha=0.2),  \n",
    "        Dropout(rate=0.25),  \n",
    "        Conv1D(filters=64, kernel_size=k_size),  \n",
    "        LeakyReLU(alpha=0.2),  \n",
    "        Dropout(rate=0.25),  \n",
    "        Flatten(),  # 1차원으로 평탄화\n",
    "        Dense(units=1)  # 1개의 유닛을 가진 Dense 레이어\n",
    "    ])\n",
    "    return Model(x, model(x))\n",
    "\n",
    "print(\"critic_x 레이어 생성 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff2209",
   "metadata": {},
   "source": [
    "- Conv1D : \n",
    "    - kernel_size : 합성곱 연산 수행 시의 사용하는 커널의 크기\n",
    "    - 커널 사이즈 2 : 작은 구간에서 특징 추출. 세부적인 패턴 인식. 인접한 두 타임 스텝에서 특징 추출\n",
    "    - 커널 사이즈 5 : 더 넓은 구간서 추출. 큰 패턴 트렌드 인식. 인접 다섯 타임 스텝에서 특징 추출\n",
    "    - 커널이 클수록 많은 계산 필요로 함\n",
    "- LeakyReLU : 활성화함수 중 하나\n",
    "    - 음수 입력에 대해서도 작은 기울기를 갖게 하여 죽은 뉴런 문제를 해결\n",
    "    - alpha : 음수입력에 대한 기울기. 보통 0.01 혹은 0.2\n",
    "    - 너무 큰 alpha 값은 음수 입력에 대한 기울기를 크게 하여 모델의 성능 저하\n",
    "    - 너무 작은 alpha는 ReLU와 다를 게 없음\n",
    "    - ReLU는 alpha=0인 경우\n",
    "- ReLU : ReLU 계열의 활성화함수의 기반이 되는 활성화함수\n",
    "    - 간단하고 계산 비용이 적으며, 기울기 소실 문제를 해결하는 활성화함수\n",
    "    - 학습 속도와 예측 속도가 빠르며, 깊은 신경망에서도 효과적\n",
    "    - 죽은 뉴런 문제와 출력의 비대칭성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71168bc3",
   "metadata": {},
   "source": [
    "- MTadGAN의 판별자 critic_z 레이어를 만드는 함수를 정의한다. critic_z는 실제의 학습 데이터 샘플로부터 encoding 된 벡터와 그냥 숨은 공간에서 샘플된 벡터 사이를 구별해내는 역할을 한다. 구별해내는 판별자 네트워크를 말한다. 즉, 그림 19에서 보듯이 무작위로 택한 잠재공간 내의 표본 z 와 학습 데이터 샘플로부터 생성자 E(x) 를 써서 인코딩(encoding)된 표본을 분별하는 함수(네트워크)이다. 즉 그림 19에서 보듯이 critic_z는 잠재공간(embedding space) 내에서의 판별자이고 critic_x는 이와 반대로 실제 데이터 공간에서의 판별자가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca3c7673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_z 레이어 생성 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def build_critic_z_layer(input_shape):\n",
    "    \"\"\"critic_z 레이어 생성\n",
    "    Args:\n",
    "    critic_z_input_shape: [20, 1]\n",
    "    Returns:\n",
    "    critic_z 모델\n",
    "    \"\"\"\n",
    "    x = Input(shape=input_shape)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        Flatten(),  # 입력을 1차원으로 평탄화\n",
    "        Dense(units=100),  # 100개의 유닛을 가진 Dense 레이어\n",
    "        LeakyReLU(alpha=0.2),  # LeakyReLU 활성화 함수, 알파=0.2\n",
    "        Dropout(rate=0.2),  # 드롭아웃 20%\n",
    "        Dense(units=100),  \n",
    "        LeakyReLU(alpha=0.2), \n",
    "        Dropout(rate=0.2),  \n",
    "        Dense(units=1)  # 1개의 유닛을 가진 Dense 레이어\n",
    "    ])\n",
    "    return Model(x, model(x))\n",
    "\n",
    "print(\"critic_z 레이어 생성 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea5628",
   "metadata": {},
   "source": [
    "- MTadGAN의 Wasserstein 로스 함수를 정의한다. Wasserstein 로스 함수는 두 확률분포 사이의 거리를 주는 함수로서 판별자 네트워크의 학습에 쓰이는 로스함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f716d12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein 손실 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\"Wasserstein 손실 계산\n",
    "    Args:\n",
    "    y_true: 실제 값\n",
    "    y_pred: 예측 값\n",
    "    Returns:\n",
    "    loss: Wasserstein 손실\n",
    "    \"\"\"\n",
    "    return K.mean(y_true * y_pred)\n",
    "print(\"Wasserstein 손실 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe6ac9",
   "metadata": {},
   "source": [
    "- MTadGAN에서 필요로 하는 checkpoints 디렉터리, 기타 앞에서 정의한 네트워크 파라미터 값을 복사한다. checkpoints 디렉터리는 학습된 MTadGAN 모형을 저장하는 데에 쓰인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedcc927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTadGAN 초기화\n",
      "latent_dim=  20\n",
      "shape=  [10, 3]\n",
      "encoder_input_shape=  [10, 3]\n",
      "generator_input_shape=  [20, 1]\n",
      "critic_x_input_shape=  [10, 3]\n",
      "critic_z_input_shape=  [20, 1]\n",
      "encoder_reshape_shape=  [20, 1]\n",
      "generator_reshape_shape=  [10, 1]\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(os.getcwd(), 'checkpoints')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "network_dir = os.path.join(os.getcwd(), 'networks')\n",
    "os.makedirs(network_dir, exist_ok=True)\n",
    "# 네트워크 플랏\n",
    "plot_network = params['plot_network']\n",
    "# 학습 파라미터\n",
    "batch_size = params['batch_size']\n",
    "n_critics = params['n_critic']\n",
    "epochs = params['epochs']\n",
    "# 층 파라미터\n",
    "shape = params['shape']\n",
    "window_size = shape[0]\n",
    "feat_dim = shape[1]\n",
    "latent_dim = params['latent_dim']\n",
    "encoder_input_shape = params['encoder_input_shape']\n",
    "generator_input_shape = params['generator_input_shape']\n",
    "critic_x_input_shape = params['critic_x_input_shape']\n",
    "critic_z_input_shape = params['critic_z_input_shape']\n",
    "encoder_reshape_shape = params['encoder_reshape_shape']\n",
    "generator_reshape_shape = params['generator_reshape_shape']\n",
    "print('MTadGAN 초기화')\n",
    "print(\"latent_dim= \", latent_dim)\n",
    "print(\"shape= \", shape)\n",
    "print(\"encoder_input_shape= \", encoder_input_shape)\n",
    "print(\"generator_input_shape= \", generator_input_shape)\n",
    "print(\"critic_x_input_shape= \", critic_x_input_shape)\n",
    "print(\"critic_z_input_shape= \", critic_z_input_shape)\n",
    "print(\"encoder_reshape_shape= \", encoder_reshape_shape)\n",
    "print(\"generator_reshape_shape= \", generator_reshape_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514ccc5",
   "metadata": {},
   "source": [
    "- MTadGAN의 encoder, generator 및 판별자(critic_x, critic_z) 네트워크의 인스턴스를 정의한다. 동시에 학습 계산을 위한 최적화 알고리즘으로 Adam 함수를 도입한다. 학습률의 값은 learning_rate = 0.0005로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c4fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder generator critic_x critic_z optimizer 인스턴스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "learning_rate =0.0005\n",
    "# 각 함수에 입력 층 부여\n",
    "encoder = build_encoder_layer(input_shape=encoder_input_shape,\n",
    "                              encoder_reshape_shape=encoder_reshape_shape)\n",
    "generator = build_generator_layer(input_shape=generator_input_shape,\n",
    "                                  generator_reshape_shape=generator_reshape_shape)\n",
    "critic_x = build_critic_x_layer(input_shape=critic_x_input_shape)\n",
    "critic_z = build_critic_z_layer(input_shape=critic_z_input_shape)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "print(\"encoder generator critic_x critic_z optimizer 인스턴스 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a40d0",
   "metadata": {},
   "source": [
    "- 판별자 네트워크와 encoder-generator 네트워크의 입출력 구조를 명시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba19bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Input(shape=(latent_dim, 1))\n",
    "x = Input(shape=shape)\n",
    "x_ = generator(z)\n",
    "z_ = encoder(x)\n",
    "fake_x = critic_x(x_)\n",
    "valid_x = critic_x(x)\n",
    "interpolated_x = RandomWeightedAverage(batch_size)([x, x_])\n",
    "critic_x_model = Model(inputs=[x, z], outputs=[valid_x, fake_x, interpolated_x])\n",
    "fake_z = critic_z(z_)\n",
    "valid_z = critic_z(z)\n",
    "interpolated_z = RandomWeightedAverage(batch_size)([z, z_])\n",
    "critic_z_model = Model(inputs=[x, z], outputs=[valid_z, fake_z, interpolated_z])\n",
    "z_gen = Input(shape=(latent_dim, 1))\n",
    "x_gen_ = generator(z_gen)\n",
    "x_gen = Input(shape=shape)\n",
    "z_gen_ = encoder(x_gen)\n",
    "x_gen_rec = generator(z_gen_)\n",
    "fake_gen_x = critic_x(x_gen_)\n",
    "fake_gen_z = critic_z(z_gen_)\n",
    "encoder_generator_model = Model([x_gen, z_gen], [fake_gen_x, fake_gen_z, x_gen_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231b28a",
   "metadata": {},
   "source": [
    "- z는 잠재 공간(latent space)에서 입력을 받는 텐서. 입력 형태는 (latent_dim, 1)\n",
    "- x는 실제 데이터 입력을 받는 텐서입니다. 입력 형태는 shape로 정의\n",
    "- x_는 생성기(generator)를 통해 잠재 공간 z로부터 생성된 가짜 데이터\n",
    "- z_는 실제 데이터 x를 인코더(encoder)를 통해 변환한 잠재 표현\n",
    "- fake_x는 critic_x 모델이 생성된 가짜 데이터 x_에 대해 평가한 결과\n",
    "- valid_x는 critic_x 모델이 실제 데이터 x에 대해 평가한 결과\n",
    "- interpolated_x는 실제 데이터 x와 생성된 데이터 x_의 랜덤 가중 평균. WGAN-GP(Wasserstein GAN with Gradient Penalty)에서 사용\n",
    "- critic_x_model은 critic_x의 모델로, 실제 데이터 x, 잠재 공간 z를 입력으로 받아 valid_x, fake_x, interpolated_x를 출력으로 함\n",
    "- fake_z는 critic_z 모델이 인코더로부터 얻은 잠재 표현 z_에 대해 평가한 결과\n",
    "-  valid_z는 critic_z 모델이 잠재 공간 z에 대해 평가한 결과\n",
    "- z_gen은 생성기에서 입력을 받는 잠재 공간 z를 나타냄. 입력 형태는 (latent_dim, 1)\n",
    "- x_gen_는 생성기를 통해 잠재 공간 z_gen에서 생성된 가짜 데이터\n",
    "- x_gen은 실제 데이터의 입력을 받는 텐서. 입력 형태는 shape로 정의\n",
    "- z_gen_는 실제 데이터 x_gen을 인코더를 통해 변환한 잠재 표현\n",
    "- x_gen_rec는 인코더를 통해 얻은 잠재 표현 z_gen_를 다시 생성기를 통해 복원한 데이터. 재구성된 데이터\n",
    "- fake_gen_x는 critic_x 모델이 생성된 가짜 데이터 x_gen_에 대해 평가한 결과\n",
    "- fake_gen_z는 critic_z 모델이 인코더를 통해 얻은 잠재 표현 z_gen_에 대해 평가한 결과\n",
    "- ncoder_generator_model은 인코더와 생성기를 결합한 모델로, 실제 데이터 x_gen과 잠재 공간 z_gen을 입력으로 받아 가짜 데이터에 대한 평가 결과 fake_gen_x, 잠재 표현에 대한 평가 결과 fake_gen_z, 그리고 재구성된 데이터 x_gen_rec을 출력으로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9d456",
   "metadata": {},
   "source": [
    "- 이미 critic_x 모형, critic_z 모형 그리고 encoder-generator 모형의 세 가지가 학습되어 저장되어 있으면 이를 불러오고, 네트워크 구조를 그래픽 파일로 저장하며 입출력 구조를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a91aad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 10, 3)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 10, 3)        133205      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Functional)            (None, 1)            25601       model_1[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "random_weighted_average (Random (64, 10, 3)          0           input_6[0][0]                    \n",
      "                                                                 model_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 158,806\n",
      "Trainable params: 158,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 10, 3)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 20, 1)        5140        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Functional)            (None, 1)            12301       model[0][0]                      \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "random_weighted_average_1 (Rand (64, 20, 1)          0           input_5[0][0]                    \n",
      "                                                                 model[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 17,441\n",
      "Trainable params: 17,441\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 10, 3)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 10, 3)        133205      input_7[0][0]                    \n",
      "                                                                 model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 20, 1)        5140        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Functional)            (None, 1)            25601       model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Functional)            (None, 1)            12301       model[1][0]                      \n",
      "==================================================================================================\n",
      "Total params: 176,247\n",
      "Trainable params: 176,247\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "critic_x_model critic_z_model encode_generator_model 정의 완료\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(os.path.join(ckpt_dir, 'critic_x_model.h5')):\n",
    "    print(\"load critic_x weights\")\n",
    "    critic_x_model.load_weights(os.path.join(ckpt_dir, 'critic_x_model.h5'))\n",
    "if os.path.isfile(os.path.join(ckpt_dir, 'critic_z_model.h5')):\n",
    "    print(\"load critic_z weights\")\n",
    "    critic_z_model.load_weights(os.path.join(ckpt_dir, 'critic_z_model.h5'))\n",
    "if os.path.isfile(os.path.join(ckpt_dir, 'encoder_generator_model.h5')):\n",
    "    print(\"load encoder_generator weights\")\n",
    "    encoder_generator_model.load_weights(os.path.join(\n",
    "                                            ckpt_dir,\n",
    "                                            'encoder_generator_model.h5'))\n",
    "critic_x_model.summary()\n",
    "critic_z_model.summary()\n",
    "encoder_generator_model.summary()\n",
    "\n",
    "if plot_network:\n",
    "    plot_model(critic_x_model,\n",
    "            to_file=os.path.join(network_dir, 'critic_x_model_tf2.png'),\n",
    "            show_shapes=True,\n",
    "            expand_nested=True)\n",
    "    plot_model(critic_z_model,\n",
    "        to_file=os.path.join(network_dir, 'critic_z_model_tf2.png'),\n",
    "        show_shapes=True,\n",
    "        expand_nested=True)\n",
    "    plot_model(encoder_generator_model,\n",
    "        to_file=os.path.join(network_dir,'enc_gen_model_tf2.png'),\n",
    "        show_shapes=True,\n",
    "        expand_nested=True)\n",
    "print(\"critic_x_model critic_z_model encode_generator_model 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ceda21",
   "metadata": {},
   "source": [
    "- critic_x 네트워크를 학습하는 과정을 수행하는 함수를 정의한다. 로스함수를 정의하고 gradient 계산함수, 그리고 Adam 최적화 함수를 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e064172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_x_train_on_batch 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def critic_x_train_on_batch(x, z, valid, fake, delta):\n",
    "    # GradientTape를 사용하여 그래디언트를 추적\n",
    "    with tf.GradientTape() as tape:\n",
    "        # critic_x_model을 통해 valid_x, fake_x, interpolated 값을 계산 (training=True 설정)\n",
    "        (valid_x, fake_x, interpolated) = critic_x_model(inputs=[x, z], training=True)\n",
    "        \n",
    "        # GradientTape를 사용하여 interpolated의 그래디언트를 추적\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = critic_x(interpolated, training=True)\n",
    "        \n",
    "        # interpolated에 대한 그래디언트를 계산\n",
    "        grads = gp_tape.gradient(pred, interpolated)[0]\n",
    "        grads = tf.square(grads)\n",
    "        ddx = tf.sqrt(1e-8 + tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
    "        \n",
    "        # Gradient Penalty 손실 계산\n",
    "        gp_loss = tf.reduce_mean((ddx - 1.0) ** 2)\n",
    "        \n",
    "        # Wasserstein 손실 계산\n",
    "        loss = tf.reduce_mean(wasserstein_loss(valid, valid_x))\n",
    "        loss += tf.reduce_mean(wasserstein_loss(fake, fake_x))\n",
    "        \n",
    "        # 최종 손실에 Gradient Penalty 손실을 추가\n",
    "        loss += gp_loss * 10.0\n",
    "    \n",
    "    # 손실에 대한 critic_x_model의 가중치 그래디언트를 계산\n",
    "    gradients = tape.gradient(loss, critic_x_model.trainable_weights)\n",
    "    \n",
    "    # 옵티마이저를 사용하여 가중치 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, critic_x_model.trainable_weights))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"critic_x_train_on_batch 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cca45",
   "metadata": {},
   "source": [
    "- critic_z 네트워크를 학습하는 과정을 수행하는 함수를 정의한다. 로스함수를 정의하고 gradient 계산함수, 그리고 Adam 최적화 함수를 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7eb3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_z_train_on_batch 정의 완료\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def critic_z_train_on_batch(x, z, valid, fake, delta):\n",
    "    # GradientTape를 사용하여 그래디언트를 추적\n",
    "    with tf.GradientTape() as tape:\n",
    "        # critic_z_model을 통해 valid_z, fake_z, interpolated 값을 계산 (training=True 설정)\n",
    "        (valid_z, fake_z, interpolated) = critic_z_model(inputs=[x, z], training=True)\n",
    "        \n",
    "        # GradientTape를 사용하여 interpolated의 그래디언트를 추적\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = critic_z(interpolated, training=True)\n",
    "        \n",
    "        # interpolated에 대한 그래디언트를 계산\n",
    "        grads = gp_tape.gradient(pred, interpolated)[0]\n",
    "        grads = tf.square(grads)\n",
    "        ddx = tf.sqrt(1e-8 + tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
    "        \n",
    "        # Gradient Penalty 손실 계산\n",
    "        gp_loss = tf.reduce_mean((ddx - 1.0) ** 2)\n",
    "        \n",
    "        # Wasserstein 손실 계산\n",
    "        loss = tf.reduce_mean(wasserstein_loss(valid, valid_z))\n",
    "        loss += tf.reduce_mean(wasserstein_loss(fake, fake_z))\n",
    "        \n",
    "        # 최종 손실에 Gradient Penalty 손실을 추가\n",
    "        loss += gp_loss * 10.0\n",
    "    \n",
    "    # 손실에 대한 critic_z_model의 가중치 그래디언트를 계산\n",
    "    gradients = tape.gradient(loss, critic_z_model.trainable_weights)\n",
    "    \n",
    "    # 옵티마이저를 사용하여 가중치 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, critic_z_model.trainable_weights))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"critic_z_train_on_batch 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c8a09",
   "metadata": {},
   "source": [
    "- encoder-generator 네트워크를 학습하는 과정을 수행하는 함수를 정의한다. 로스함수를 정의하고 gradient 함수, 그리고 Adam 최적화 함수를 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8cc8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_gen_train_on_batch 정의 완료\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def enc_gen_train_on_batch(x, z, valid):\n",
    "    # GradientTape를 사용하여 그래디언트를 추적\n",
    "    with tf.GradientTape() as tape:\n",
    "        # encoder_generator_model을 통해 fake_gen_x, fake_gen_z, x_gen_rec 값을 계산 (training=True 설정)\n",
    "        (fake_gen_x, fake_gen_z, x_gen_rec) = encoder_generator_model(inputs=[x, z], training=True)\n",
    "        \n",
    "        # 실제 데이터 x와 재구성된 데이터 x_gen_rec을 차원 축소\n",
    "        x = tf.squeeze(x)\n",
    "        x_gen_rec = tf.squeeze(x_gen_rec)\n",
    "        \n",
    "        # Wasserstein 손실 계산\n",
    "        loss = tf.reduce_mean(wasserstein_loss(valid, fake_gen_x))\n",
    "        loss += tf.reduce_mean(wasserstein_loss(valid, fake_gen_z))\n",
    "        \n",
    "        # 재구성 손실(MSE) 계산 및 추가\n",
    "        loss += tf.keras.losses.MSE(x, x_gen_rec) * 10\n",
    "        \n",
    "        # 손실 평균 계산\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    # 손실에 대한 encoder_generator_model의 가중치 그래디언트를 계산\n",
    "    gradients = tape.gradient(loss, encoder_generator_model.trainable_weights)\n",
    "    \n",
    "    # 옵티마이저를 사용하여 가중치 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, encoder_generator_model.trainable_weights))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"enc_gen_train_on_batch 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5bf5d",
   "metadata": {},
   "source": [
    "- 학습률 Learning rate = 0.0005, 반복 횟수 epochs=30으로 학습을 진행하며 각 epoch 끝날 때마다 로스 함수들을 출력한다. 학습 부분은 로스 함수의 최적화 과정에서 초기 네트워크 상태를 무작위 상태로부터 시작하거나 또는 기존의 네트워크의 모형을 불러와서 시작하거나 하므로 그 시작 상태에 따라 출력 결과가 실행 시마다 조금씩 달라진다. 즉, 최종적으로 학습된 네트워크 모형이 달라질 수 있는 것이다. 따라서 이렇게 조금씩 달라진 네트워크 모형을 써서 수행하는 테스트 결과 또한 실행 시마다 조금씩 그 결과가 달라진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1a24a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30, [Dx loss: -0.5098100304603577] [Dz loss: -8.854000091552734] [G loss: 12.369721412658691]\n",
      "Epoch: 2/30, [Dx loss: 0.32244664430618286] [Dz loss: -7.961201190948486] [G loss: 17.240400314331055]\n",
      "Epoch: 3/30, [Dx loss: 0.3969837725162506] [Dz loss: -6.578747749328613] [G loss: 16.552921295166016]\n",
      "Epoch: 4/30, [Dx loss: 0.17430128157138824] [Dz loss: -6.177322864532471] [G loss: 11.285940170288086]\n",
      "Epoch: 5/30, [Dx loss: 0.024358348920941353] [Dz loss: -6.293560981750488] [G loss: 13.72192096710205]\n",
      "Epoch: 6/30, [Dx loss: -0.5393539667129517] [Dz loss: -7.561406135559082] [G loss: 14.157665252685547]\n",
      "Epoch: 7/30, [Dx loss: 0.014545773155987263] [Dz loss: -9.815912246704102] [G loss: 11.349740982055664]\n",
      "Epoch: 8/30, [Dx loss: 0.14468812942504883] [Dz loss: -9.328973770141602] [G loss: 8.92959213256836]\n",
      "Epoch: 9/30, [Dx loss: -0.7452656030654907] [Dz loss: -8.527708053588867] [G loss: 0.8259146809577942]\n",
      "Epoch: 10/30, [Dx loss: -0.29398059844970703] [Dz loss: -7.268407344818115] [G loss: -10.538219451904297]\n",
      "Epoch: 11/30, [Dx loss: 0.6125052571296692] [Dz loss: -6.314779281616211] [G loss: -11.740966796875]\n",
      "Epoch: 12/30, [Dx loss: -0.000302667758660391] [Dz loss: -6.197141647338867] [G loss: -6.56245231628418]\n",
      "Epoch: 13/30, [Dx loss: 0.26982468366622925] [Dz loss: -7.240180969238281] [G loss: 0.36593741178512573]\n",
      "Epoch: 14/30, [Dx loss: -0.07704676687717438] [Dz loss: -7.611861228942871] [G loss: 8.914176940917969]\n",
      "Epoch: 15/30, [Dx loss: 0.25490739941596985] [Dz loss: -7.036896228790283] [G loss: 12.090999603271484]\n",
      "Epoch: 16/30, [Dx loss: 1.023648977279663] [Dz loss: -6.876272201538086] [G loss: 14.010637283325195]\n",
      "Epoch: 17/30, [Dx loss: 0.56172114610672] [Dz loss: -5.9402995109558105] [G loss: 18.295291900634766]\n",
      "Epoch: 18/30, [Dx loss: -1.4265429973602295] [Dz loss: -5.885190010070801] [G loss: 15.30832290649414]\n",
      "Epoch: 19/30, [Dx loss: -0.06203961744904518] [Dz loss: -5.359879493713379] [G loss: 9.599079132080078]\n",
      "Epoch: 20/30, [Dx loss: 1.5105061531066895] [Dz loss: -6.213987350463867] [G loss: 6.110494613647461]\n",
      "Epoch: 21/30, [Dx loss: -0.21658992767333984] [Dz loss: -6.175461769104004] [G loss: 10.062737464904785]\n",
      "Epoch: 22/30, [Dx loss: -0.25449222326278687] [Dz loss: -7.6630539894104] [G loss: 15.984392166137695]\n",
      "Epoch: 23/30, [Dx loss: 0.524779200553894] [Dz loss: -7.731682777404785] [G loss: 9.815088272094727]\n",
      "Epoch: 24/30, [Dx loss: -0.6938127875328064] [Dz loss: -7.50180196762085] [G loss: -1.6871010065078735]\n",
      "Epoch: 25/30, [Dx loss: 0.32794514298439026] [Dz loss: -7.71790075302124] [G loss: -2.047060012817383]\n",
      "Epoch: 26/30, [Dx loss: 0.06360188126564026] [Dz loss: -7.871487617492676] [G loss: 4.083288192749023]\n",
      "Epoch: 27/30, [Dx loss: 0.16152456402778625] [Dz loss: -8.004508018493652] [G loss: 10.703205108642578]\n",
      "Epoch: 28/30, [Dx loss: 1.1112868785858154] [Dz loss: -8.51130199432373] [G loss: 8.300833702087402]\n",
      "Epoch: 29/30, [Dx loss: 0.2626669704914093] [Dz loss: -7.855023384094238] [G loss: 1.127974271774292]\n",
      "Epoch: 30/30, [Dx loss: -0.02186974696815014] [Dz loss: -7.170130729675293] [G loss: -6.308717250823975]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py when saving in hdf5.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(epoch_g_loss), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, [Dx loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] [Dz loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, epochs, cx_loss, cz_loss, g_loss)) \n\u001b[1;32m---> 36\u001b[0m \u001b[43mcritic_x_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcritic_x_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m critic_z_model\u001b[38;5;241m.\u001b[39msave_weights(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ckpt_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_z_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m), save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m encoder_generator_model\u001b[38;5;241m.\u001b[39msave_weights(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ckpt_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_generator_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m), save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2205\u001b[0m, in \u001b[0;36mModel.save_weights\u001b[1;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[0;32m   2198\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2199\u001b[0m       (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_weights got save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2200\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepath (\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) looks like an HDF5 file. Omit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2201\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhen saving in TensorFlow format.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2202\u001b[0m       \u001b[38;5;241m%\u001b[39m filepath)\n\u001b[0;32m   2204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2205\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2206\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`save_weights` requires h5py when saving in hdf5.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   2208\u001b[0m   check_filepath \u001b[38;5;241m=\u001b[39m filepath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: `save_weights` requires h5py when saving in hdf5."
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "X = X.reshape((-1, shape[0], feat_dim))  # feat_dim = 주성분 차원\n",
    "X_ = np.copy(X)\n",
    "fake = np.ones((batch_size, 1), dtype=np.float32)  # 가짜 레이블 (1)\n",
    "valid = -np.ones((batch_size, 1), dtype=np.float32)  # 진짜 레이블 (-1)\n",
    "delta = np.ones((batch_size, 1), dtype=np.float32)  # 델타 (1)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    np.random.shuffle(X_)  # 데이터를 섞음\n",
    "    epoch_g_loss = []  # 생성기 손실을 저장할 리스트\n",
    "    epoch_cx_loss = []  # critic_x 손실을 저장할 리스트\n",
    "    epoch_cz_loss = []  # critic_z 손실을 저장할 리스트\n",
    "    minibatches_size = batch_size * n_critics\n",
    "    num_minibatches = int(X_.shape[0] // minibatches_size)\n",
    "    for i in range(num_minibatches):\n",
    "        minibatch = X_[i * minibatches_size:(i + 1) * minibatches_size]\n",
    "        # 크리틱 학습\n",
    "        critic_x.trainable = True\n",
    "        critic_z.trainable = True\n",
    "        generator.trainable = False\n",
    "        encoder.trainable = False\n",
    "        for j in range(n_critics):\n",
    "            x = minibatch[j * batch_size:(j + 1) * batch_size]\n",
    "            z = np.random.normal(size=(batch_size, latent_dim, 1))\n",
    "            epoch_cx_loss.append(critic_x_train_on_batch(x, z, valid, fake, delta))\n",
    "            epoch_cz_loss.append(critic_z_train_on_batch(x, z, valid, fake, delta))\n",
    "        critic_x.trainable = False\n",
    "        critic_z.trainable = False \n",
    "        generator.trainable = True\n",
    "        encoder.trainable = True \n",
    "        # 인코더와 생성기 학습\n",
    "        epoch_g_loss.append(enc_gen_train_on_batch(x, z, valid))\n",
    "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "    print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [G loss: {}]'.format(epoch, epochs, cx_loss, cz_loss, g_loss)) \n",
    "critic_x_model.save_weights(os.path.join(ckpt_dir, 'critic_x_model.h5'), save_format='h5')\n",
    "critic_z_model.save_weights(os.path.join(ckpt_dir, 'critic_z_model.h5'), save_format='h5')\n",
    "encoder_generator_model.save_weights(os.path.join(ckpt_dir, 'encoder_generator_model.h5'), save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f09556",
   "metadata": {},
   "source": [
    "## [단계 ⑥] 테스트 데이터에 대한 계산 수행\n",
    "⑥-1. 테스트 데이터에 대한 계산 수행\n",
    "- 예측 함수 predict()를 정의한다. 이 함수는 입력 데이터에 encoder-generator 네트워크를 작용하여 재현된 데이터 y_hat을 출력하고 또한 critic_x 판별자 값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f0b124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict() 정의: 완료\n"
     ]
    }
   ],
   "source": [
    "def predict(X):\n",
    "    \"\"\"초기 설정된 객체를 사용하여 값을 예측.\n",
    "    Args:\n",
    "    X (ndarray): 모델을 위한 입력 시퀀스를 포함하는 N차원 배열.\n",
    "    Returns:\n",
    "    ndarray:\n",
    "    각 입력 시퀀스에 대한 재구성을 포함하는 N차원 배열.\n",
    "    ndarray:\n",
    "    각 입력 시퀀스에 대한 비평 점수를 포함하는 N차원 배열.\n",
    "    \"\"\"\n",
    "    X = X.reshape((-1, shape[0], feat_dim)) # feat_dim : 특징 차원 \n",
    "    z_ = encoder.predict(X)\n",
    "    y_hat = generator.predict(z_)\n",
    "    critic = critic_x.predict(X)\n",
    "    return y_hat, critic\n",
    "print(\"predict() 정의: 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454a2d8",
   "metadata": {},
   "source": [
    "- 미리 준비된 테스트 데이터 파일 “./data/preprocessed/test/Test07_NG_dchg.csv”와 anomaly 파일인 “./data/preprocessed/test/Test07_NG_dchg_Label.csv”를 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e89d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argments for prediction mode\n",
      "----------------------\n",
      "args.timest_form : 0\n",
      "args.signal_file : ../../../../data/preprocessed/test/Test07_NG_dchg.csv\n",
      "args.anomaly_file : ../../../../data/preprocessed/test/Test07_NG_dchg_Label.csv\n",
      "args.mode : predict\n",
      "args.aggregate_interval : 1\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 이용한 예측\n",
    "# 테스트 데이터 불러오기\n",
    "args=arguments(signal_file='../../../../data/preprocessed/test/Test07_NG_dchg.csv',\n",
    "    timest_form=0,\n",
    "    anomaly_file='../../../../data/preprocessed/test/Test07_NG_dchg_Label.csv',\n",
    "    mode='predict',\n",
    "    aggregate_interval=1,\n",
    "    regate_interval=1)\n",
    "print(\"argments for prediction mode\")\n",
    "print(\"----------------------\")\n",
    "print(\"args.timest_form :\", args.timest_form)\n",
    "print(\"args.signal_file :\", args.signal_file)\n",
    "print(\"args.anomaly_file :\", args.anomaly_file)\n",
    "print(\"args.mode :\", args.mode)\n",
    "print(\"args.aggregate_interval :\", args.aggregate_interval)\n",
    "print(\"----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1102260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = args.signal_file \n",
    "df_test1 = pd.read_csv(file_path) # 테스트 데이터 데이터 프레임 형태로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fc09b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA reduction of test data df.shape =  (4594, 4)\n",
      "----------------------\n",
      "   date     pca_1     pca_2     pca_3\n",
      "0     1 -0.412384  0.583294  0.027636\n",
      "1     2 -0.406828  0.567897  0.026531\n",
      "2     3 -0.368964  0.575908  0.015944\n",
      "3     4 -0.430935  0.547518  0.026097\n",
      "4     5 -0.425375  0.532121  0.025034\n",
      "----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## PCA를 이용한 차원 축소 ########\n",
    "data_0 = df_test1 # 복제\n",
    "# (lags_n=0, diffs_n=0, smooth_n=0)\n",
    "data_1 = diff_smooth_df(data_0, lags_n, diffs_n, smooth_n)\n",
    "pca = PCA(n_components=features_dim)\n",
    "data = pca.fit_transform(data_1) # PCA를 이용한 차원 축소\n",
    "df_1 = []\n",
    "for i in range(len(data)):\n",
    "    row=[i+1] # 정수 인덱스 부여\n",
    "    for jj in range(features_dim):\n",
    "        row.append(data[i][jj])\n",
    "    df_1.append(row)\n",
    "df = pd.DataFrame(df_1)\n",
    "columns_new = ['date']\n",
    "for i in range(1, features_dim+1):\n",
    "    pca_i = 'pca_%s' % str(i)\n",
    "    columns_new.append(pca_i)\n",
    "df.columns=columns_new\n",
    "print(\"After PCA reduction of test data df.shape = \", df.shape)\n",
    "print(\"----------------------\")\n",
    "print(df.head(5))\n",
    "print(\"----------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd581b",
   "metadata": {},
   "source": [
    "- MTadGAN 모형에 입력하기 위하여 테스트 데이터에 대한 집합화(aggregate)를 수행한다. time_segments_aggregate() 함수에 대해서는 위의 코드를 참조한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "738dcaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test signal data (after time_segments_aggregate) =  (4594, 3)\n",
      "----------------------\n",
      "[[-0.41238433  0.58329384  0.02763632]\n",
      " [-0.40682767  0.56789719  0.02653118]\n",
      " [-0.36896407  0.57590827  0.01594388]\n",
      " [-0.43093477  0.54751791  0.02609742]\n",
      " [-0.42537477  0.532121    0.02503421]]\n",
      "----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 시간 집합화 수행\n",
    "#\n",
    "X, index = time_segments_aggregate(df, interval=args.aggregate_interval, time_column='date')\n",
    "print(\"Test signal data (after time_segments_aggregate) = \", X.shape)\n",
    "print(\"----------------------\")\n",
    "print(X[:5])\n",
    "print(\"----------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b2de2",
   "metadata": {},
   "source": [
    "- 스케일링에 의한 정규화 과정을 수행한다. 테스트 데이터를 [-1,+1] 구간 안의 값으로 정규화한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8777364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X (after MinMaxScaler) =  (4594, 3)\n",
      "----------------------\n",
      "[[-0.98179729  1.          0.02048555]\n",
      " [-0.97807573  0.96581515  0.01806001]\n",
      " [-0.95271668  0.98360199 -0.0051768 ]\n",
      " [-0.9942214   0.9205675   0.01710801]\n",
      " [-0.99049761  0.88638206  0.01477449]]\n",
      "----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최대 - 최소 정규화 적용\n",
    "X = SimpleImputer().fit_transform(X)\n",
    "X = MinMaxScaler(feature_range=(-1, 1)).fit_transform(X)\n",
    "print(\"Test X (after MinMaxScaler) = \", X.shape)\n",
    "print(\"----------------------\")\n",
    "print(X[:5])\n",
    "print(\"----------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ec687",
   "metadata": {},
   "source": [
    "- MTadGAN의 구조에 맞게 입력할 수 있도록 테스트 데이터에 rolling_window_sequences() 함수를 적용하여 이웃하는 데이터 값으로 윈도우 길이(win_size=10)만큼씩 묶어서 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfb8d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (after rolling_window_seq): (4584, 10, 3)\n",
      "X : \n",
      "[[[-9.81797293e-01  1.00000000e+00  2.04855463e-02]\n",
      "  [-9.78075731e-01  9.65815151e-01  1.80600130e-02]\n",
      "  [-9.52716680e-01  9.83601990e-01 -5.17680324e-03]\n",
      "  [-9.94221404e-01  9.20567495e-01  1.71080075e-02]\n",
      "  [-9.90497609e-01  8.86382057e-01  1.47744897e-02]]\n",
      "\n",
      " [[-9.78075731e-01  9.65815151e-01  1.80600130e-02]\n",
      "  [-9.52716680e-01  9.83601990e-01 -5.17680324e-03]\n",
      "  [-9.94221404e-01  9.20567495e-01  1.71080075e-02]\n",
      "  [-9.90497609e-01  8.86382057e-01  1.47744897e-02]\n",
      "  [-9.60544141e-01  9.71167281e-01 -9.41994759e-04]]\n",
      "\n",
      " [[-9.52716680e-01  9.83601990e-01 -5.17680324e-03]\n",
      "  [-9.94221404e-01  9.20567495e-01  1.71080075e-02]\n",
      "  [-9.90497609e-01  8.86382057e-01  1.47744897e-02]\n",
      "  [-9.60544141e-01  9.71167281e-01 -9.41994759e-04]\n",
      "  [-9.79474891e-01  9.25992926e-01  1.28564571e-02]]]\n",
      "X index shape: (4584,)\n",
      "y shape: (4584, 1)\n",
      "y : \n",
      "[[-0.97946451]\n",
      " [-0.99049122]\n",
      " [-0.99048524]\n",
      " [-0.97807193]\n",
      " [-0.97945812]]\n",
      "y index shape: (4584,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rolling window sequences 처리\n",
    "X, y, X_index, y_index = rolling_window_sequences(X, index, window_size= win_size,\n",
    " target_size=1, step_size=1, target_column=0 ) \n",
    "print(\"X shape (after rolling_window_seq): {}\".format(X.shape))\n",
    "print(\"X : \")\n",
    "print(X[:3, :5])\n",
    "print(\"X index shape: {}\".format(X_index.shape))\n",
    "print(\"y shape: {}\".format(y.shape))\n",
    "print(\"y : \")\n",
    "print(y[:5])\n",
    "print(\"y index shape: {}\".format(y_index.shape))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74621ff2",
   "metadata": {},
   "source": [
    "- MTadGAN에서의 Anomaly 클래스 및 이상탐지 관련 속성 함수들을 정의한다. 여기서는 Anomaly 클래스에서 주로 쓰이는 주요한 속성함수에 대하여 간략하게 설명한다. 각 함수의 입력 변수는 생략한다.  \n",
    "    - _find_threshold() : 에러 값들로부터 최적의 threshold 값을 찾는다.\n",
    "    - _fixed_threshold() : 입력 k 변수의 값에 따라 에러의 threshold를 구한다.\n",
    "    - _compute_scores() : Anomaly score를 계산한다.\n",
    "    - _find_window_sequences() : 연속된 이상 값들의 sequence를 찾는다.\n",
    "    - find_anomalies() : 이상 값을 가지는 연속된 에러값의 sequence를 찾는다.\n",
    "    - _compute_critic_score() : 판별자 score에서 얻는 이상 값의 배열을 계산한다. \n",
    "    - _reconstruction_errors() : 데이터 재현 에러 값을 계산한다. 이는 입력 데이터와 y_hat 사이의차이값으로부터 구해진다.\n",
    "    - score_anomalies() : 판별자 score와 재현 에러의 결합으로 최종 이상 score를 얻는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5dd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "시계열 이상 탐지 함수.\n",
    "일부 구현은 논문 https://arxiv.org/pdf/1802.04431.pdf을 참고하였습니다.\n",
    "\"\"\"\n",
    "class Anomaly(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _deltas(self, errors, epsilon, mean, std):\n",
    "        \"\"\"평균 및 표준편차 델타를 계산.\n",
    "        delta_mean = mean(errors) - epsilon(임계값) 이하의 모든 오류들의 평균\n",
    "        delta_std = std(errors) - epsilon(임계값) 이하의 모든 오류들의 표준편차\n",
    "        Args:\n",
    "        errors (ndarray): 오류 배열.\n",
    "        epsilon (ndarray): 임계값.\n",
    "        mean (float): 오류들의 평균.\n",
    "        std (float): 오류들의 표준편차.\n",
    "        Returns:\n",
    "        float, float:\n",
    "        * delta_mean.\n",
    "        * delta_std.\n",
    "        \"\"\"\n",
    "        below = errors[errors <= epsilon]\n",
    "        if not len(below):\n",
    "            return 0, 0\n",
    "        return mean - below.mean(), std - below.std()\n",
    "    # anomalies.py (2)\n",
    "    def _count_above(self, errors, epsilon):\n",
    "        \"\"\"epsilon 이상인 오류와 연속된 시퀀스의 수를 계산\n",
    "        연속된 시퀀스는 시프트하고 원래 값이 true였던 위치의 변화를\n",
    "        계산하여 그 위치에서 시퀀스가 시작되었음을 의미\n",
    "        Args:\n",
    "        errors (ndarray): 오류 배열.\n",
    "        epsilon (ndarray): 임계값.\n",
    "        Returns:\n",
    "        int, int:\n",
    "        * epsilon 이상인 오류의 수.\n",
    "        * epsilon 이상인 연속된 시퀀스의 수.\n",
    "        \"\"\"\n",
    "        # errors 배열에서 epsilon보다 큰 값인지 여부 배열\n",
    "        above = errors > epsilon\n",
    "        # epsilon보다 큰 오류의 총 수를 계산\n",
    "        total_above = len(errors[above])\n",
    "        # above 배열을 pandas Series로 변환\n",
    "        above = pd.Series(above)\n",
    "        # above Series를 1만큼 시프트(레코드를 한 칸씩 밈)\n",
    "        shift = above.shift(1)\n",
    "        # above와 shift된 값 간의 변화를 계산(서로 다른 경우가 true)\n",
    "        change = above != shift\n",
    "        # epsilon보다 큰 연속된 시퀀스의 수를 계산\n",
    "        total_consecutive = sum(above & change)\n",
    "        # 결과를 반환\n",
    "        return total_above, total_consecutive\n",
    "    # anomalies.py (3)\n",
    "    def _z_cost(self, z, errors, mean, std):\n",
    "        \"\"\"z 값이 얼마나 나쁜지를 계산\n",
    "        원래 공식::\n",
    "        (delta_mean/mean) + (delta_std/std)\n",
    "        ------------------------------------------------------\n",
    "        number of errors above + (number of sequences above)^2\n",
    "        이는 `z`의 \"좋음\"을 계산하며, 값이 높을수록 `z`가 더 좋다는 것을 의미\n",
    "        이 경우, 이 값을 반전(음수로 만듦)하여 비용 함수로 변환\n",
    "        나중에 scipy.fmin을 사용하여 이를 최소화\n",
    "        \n",
    "        Args:\n",
    "        z (ndarray): 비용 점수가 계산될 값.\n",
    "        errors (ndarray): 오류 배열.\n",
    "        mean (float): 오류들의 평균.\n",
    "        std (float): 오류들의 표준편차.\n",
    "        \n",
    "        Returns float: z의 비용.\n",
    "        \"\"\"\n",
    "        \n",
    "        # epsilon 값을 평균 + z * 표준편차로 계산\n",
    "        epsilon = mean + z * std\n",
    "        # epsilon을 사용하여 평균 및 표준편차 델타를 계산\n",
    "        delta_mean, delta_std = self._deltas(errors, epsilon, mean, std)\n",
    "        # epsilon보다 큰 오류와 연속된 시퀀스의 수를 계산\n",
    "        above, consecutive = self._count_above(errors, epsilon)\n",
    "        # 분자(numerator)를 계산합니다. (반전된 값)\n",
    "        numerator = -(delta_mean / mean + delta_std / std)\n",
    "        # 분모(denominator)를 계산합니다.\n",
    "        denominator = above + consecutive ** 2\n",
    "        # 분모가 0이면 무한대를 반환\n",
    "        if denominator == 0:\n",
    "            return np.inf\n",
    "        \n",
    "        # 최종 비용을 계산하여 반환\n",
    "        return numerator / denominator\n",
    "    # anomalies.py (4)\n",
    "    def _find_threshold(self, errors, z_range):\n",
    "        \"\"\"이상적인 임계값 찾는 함수.\n",
    "        이상적인 임계값은 z_cost 함수를 최소화하는 값.\n",
    "        Scipy.fmin을 사용하여 z_range의 값들을 시작점으로 최소값을 탐색.\n",
    "\n",
    "        Args:\n",
    "        errors (ndarray): 오류 배열.\n",
    "        z_range (list): scipy.fmin 함수의 시작점을 선택할 범위를 나타내는 두 값의 리스트.\n",
    "\n",
    "        Returns: float: 계산된 임계값.\n",
    "        \"\"\"\n",
    "        # 오류들의 평균을 계산.\n",
    "        mean = errors.mean()\n",
    "        \n",
    "        # 오류들의 표준편차를 계산.\n",
    "        std = errors.std()\n",
    "        \n",
    "        # z_range에서 최소값과 최대값을 가져옴.\n",
    "        min_z, max_z = z_range\n",
    "        \n",
    "        # 최적의 z값을 저장할 변수 초기화.\n",
    "        best_z = min_z\n",
    "        \n",
    "        # 최적의 비용을 무한대로 초기화.\n",
    "        best_cost = np.inf\n",
    "\n",
    "        # min_z부터 max_z까지 반복.\n",
    "        for z in range(min_z, max_z):\n",
    "            # fmin 함수를 사용하여 z에서 시작하는 최소 비용을 찾음.\n",
    "            best = fmin(self._z_cost, z, args=(errors, mean, std), full_output=True, disp=False)\n",
    "            \n",
    "            # 최적의 z값과 비용을 가져옴.\n",
    "            z, cost = best[0:2]\n",
    "            \n",
    "            # 현재 비용이 최적의 비용보다 작으면 갱신.\n",
    "            if cost < best_cost:\n",
    "                best_z = z[0]\n",
    "\n",
    "        # 최적의 임계값을 계산하여 반환.\n",
    "        return mean + best_z * std\n",
    "\n",
    "    def _fixed_threshold(self, errors, k=3.0):\n",
    "        \"\"\"임계값 계산.\n",
    "        고정된 임계값은 평균에서 k 표준편차만큼 떨어진 값으로 정의됨.\n",
    "\n",
    "        Args:\n",
    "        errors (ndarray): 오류 배열.\n",
    "\n",
    "        Returns:\n",
    "        float: 계산된 임계값.\n",
    "        \"\"\"\n",
    "        # 오류들의 평균을 계산.\n",
    "        mean = errors.mean()\n",
    "        \n",
    "        # 오류들의 표준편차를 계산.\n",
    "        std = errors.std()\n",
    "\n",
    "        # 고정된 임계값을 계산하여 반환.\n",
    "        return mean + k * std\n",
    "    \n",
    "    # anomalies.py (5-1)\n",
    "    def _find_sequences(self, errors, epsilon, anomaly_padding):\n",
    "        \"\"\"epsilon 이상인 값들의 시퀀스 탐색.\n",
    "        다음 단계들을 따름:\n",
    "        * epsilon 이상인 값을 표시하는 불리언 마스크 생성.\n",
    "        * True 값 주위의 일정 범위의 오류를 True로 표시.\n",
    "        * 이 마스크를 한 칸씩 시프트하고, 빈 공간은 False로 채움.\n",
    "        * 시프트된 마스크와 원래 마스크를 비교하여 변화가 있는지 확인.\n",
    "        * True였던 값이 변경된 지점을 시퀀스 시작으로 간주.\n",
    "        * False였던 값이 변경된 지점을 시퀀스 종료로 간주.\n",
    "\n",
    "        Args:\n",
    "        errors (ndarray): 오류 배열.\n",
    "        epsilon (float): 임계값. epsilon 이상의 모든 오류는 이상으로 간주.\n",
    "        anomaly_padding (int): 발견된 이상 주위에 추가할 오류의 개수.\n",
    "\n",
    "        Returns:\n",
    "        ndarray, float:\n",
    "        * 발견된 이상 시퀀스의 시작과 끝을 포함하는 배열.\n",
    "        * 이상으로 간주되지 않은 최대 오류 값.\n",
    "        \"\"\"\n",
    "        # epsilon 이상인 값을 표시하는 불린 마스크 생성.\n",
    "        above = pd.Series(errors > epsilon)\n",
    "\n",
    "        # True인 값의 인덱스를 찾음.\n",
    "        index_above = np.argwhere(above.values)\n",
    "\n",
    "        # True 값 주위의 일정 범위의 오류를 True로 표시.\n",
    "        for idx in index_above.flatten():\n",
    "            above[max(0, idx - anomaly_padding):min(idx + anomaly_padding + 1, len(above))] = True\n",
    "\n",
    "        # 이 마스크를 한 칸씩 시프트하고, 빈 공간은 False로 채움.\n",
    "        shift = above.shift(1).fillna(False)\n",
    "\n",
    "        # 시프트된 마스크와 원래 마스크를 비교하여 변화가 있는지 확인.\n",
    "        change = above != shift\n",
    "\n",
    "        # 모든 값이 True이면 max_below는 0으로 설정.\n",
    "        if above.all():\n",
    "            max_below = 0\n",
    "        else:\n",
    "            # 이상으로 간주되지 않은 최대 오류 값을 찾음.\n",
    "            max_below = max(errors[~above])\n",
    "\n",
    "        # 시퀀스의 시작과 끝 인덱스를 찾음.\n",
    "        index = above.index\n",
    "        starts = index[above & change].tolist()\n",
    "        ends = (index[~above & change] - 1).tolist()\n",
    "\n",
    "        # 마지막 시퀀스가 종료되지 않았을 경우 종료 인덱스를 추가.\n",
    "        if len(ends) == len(starts) - 1:\n",
    "            ends.append(len(above) - 1)\n",
    "\n",
    "        # 결과를 배열로 반환.\n",
    "        return np.array([starts, ends]).T, max_below\n",
    "\n",
    "    # anomalies.py (5-2)\n",
    "    def _get_max_errors(self, errors, sequences, max_below):\n",
    "        \"\"\"각 이상 시퀀스의 최대 오류를 가져옴.\n",
    "        또한 이상으로 간주되지 않은 최대 오류 값을 포함하는 행을 추가.\n",
    "        각 시퀀스의 최대 오류를 포함하는 ``max_error`` 열과 시작 및 종료 인덱스를\n",
    "        포함하는 ``start`` 및 ``stop`` 열이 있는 테이블을 내림차순으로 정렬하여 반환.\n",
    "\n",
    "        Args:\n",
    "        errors (ndarray): 오류 배열.\n",
    "        sequences (ndarray): 이상 시퀀스의 시작과 끝을 포함하는 배열.\n",
    "        max_below (float): 이상으로 간주되지 않은 최대 오류 값.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: ``start``, ``stop``, ``max_error`` 열을 포함하는 DataFrame 객체.\n",
    "        \"\"\"\n",
    "        # 이상으로 간주되지 않은 최대 오류 값을 포함하는 초기 값 설정.\n",
    "        max_errors = [{\n",
    "            'max_error': max_below,\n",
    "            'start': -1,\n",
    "            'stop': -1\n",
    "        }]\n",
    "\n",
    "        # 각 시퀀스에 대해 최대 오류 값을 계산하고 추가.\n",
    "        for sequence in sequences:\n",
    "            start, stop = sequence\n",
    "            sequence_errors = errors[start: stop + 1]\n",
    "            max_errors.append({\n",
    "                'start': start,\n",
    "                'stop': stop,\n",
    "                'max_error': max(sequence_errors)\n",
    "            })\n",
    "\n",
    "        # DataFrame으로 변환하고 최대 오류 값으로 내림차순 정렬.\n",
    "        max_errors = pd.DataFrame(max_errors).sort_values('max_error', ascending=False)\n",
    "\n",
    "        # 인덱스를 재설정하여 반환.\n",
    "        return max_errors.reset_index(drop=True)\n",
    "    \n",
    "    # anomalies.py (6)\n",
    "    def _prune_anomalies(self, max_errors, min_percent):\n",
    "        \"\"\"거짓 양성을 줄이기 위해 이상을 가지치기.\n",
    "        다음 단계들을 따름:\n",
    "        * 오류를 1단계 음수 방향으로 시프트하여 각 값을 다음 값과 비교.\n",
    "        * 비교하지 않기 위해 마지막 행을 제거.\n",
    "        * 각 행에 대한 백분율 증가를 계산.\n",
    "        * ``min_percent`` 이하인 행을 찾음.\n",
    "        * 이러한 행 중 가장 최근의 행의 인덱스를 찾음.\n",
    "        * 해당 인덱스 위의 모든 시퀀스 값을 가져옴.\n",
    "\n",
    "        Args:\n",
    "        max_errors (pandas.DataFrame): ``start``, ``stop``, ``max_error`` 열을 포함하는 DataFrame 객체.\n",
    "        min_percent (float):\n",
    "        이상 간의 분리를 위한 최소 백분율. 창 시퀀스에서 가장 높은 비이상 오류와의 비교.\n",
    "\n",
    "        Returns:\n",
    "        ndarray: 가지치기된 이상들의 시작, 끝, max_error를 포함하는 배열.\n",
    "        \"\"\"\n",
    "        # 오류를 1단계 음수 방향으로 시프트하여 다음 값과 비교.\n",
    "        next_error = max_errors['max_error'].shift(-1).iloc[:-1]\n",
    "\n",
    "        # 현재 오류 값을 가져옴.\n",
    "        max_error = max_errors['max_error'].iloc[:-1]\n",
    "\n",
    "        # 각 행에 대한 백분율 증가를 계산.\n",
    "        increase = (max_error - next_error) / max_error\n",
    "\n",
    "        # min_percent 이하인 행을 찾음.\n",
    "        too_small = increase < min_percent\n",
    "\n",
    "        # 모든 행이 min_percent 이하인 경우.\n",
    "        if too_small.all():\n",
    "            last_index = -1\n",
    "        else:\n",
    "            # min_percent 이상인 가장 최근의 행의 인덱스를 찾음.\n",
    "            last_index = max_error[~too_small].index[-1]\n",
    "\n",
    "        # 가지치기된 이상들의 시작, 끝, max_error를 포함하는 배열을 반환.\n",
    "        return max_errors[['start', 'stop', 'max_error']].iloc[0: last_index + 1].values\n",
    "    \n",
    "    # anomalies.py (7)\n",
    "    def _compute_scores(self, pruned_anomalies, errors, threshold, window_start):\n",
    "        \"\"\"이상의 점수를 계산.\n",
    "        시퀀스에서 최대 오류에 비례하는 점수를 계산하고, 인덱스를 절대값으로 만들기 위해 window_start 타임스탬프를 추가.\n",
    "\n",
    "        Args:\n",
    "        pruned_anomalies (ndarray): 윈도우 내 모든 이상의 시작, 끝 및 max_error를 포함하는 이상 배열.\n",
    "        errors (ndarray): 오류 배열.\n",
    "        threshold (float): 임계값.\n",
    "        window_start (int): 윈도우에서 첫 번째 오류 값의 인덱스.\n",
    "\n",
    "        Returns:\n",
    "        list: 각 이상에 대해 시작 인덱스, 종료 인덱스, 점수를 포함하는 이상 목록.\n",
    "        \"\"\"\n",
    "        anomalies = list()\n",
    "\n",
    "        # 점수 계산을 위한 분모. 오류의 평균과 표준편차의 합.\n",
    "        denominator = errors.mean() + errors.std()\n",
    "\n",
    "        # 가지치기된 각 이상에 대해 점수를 계산.\n",
    "        for row in pruned_anomalies:\n",
    "            max_error = row[2]\n",
    "\n",
    "            # 점수를 계산. (max_error - threshold) / 분모\n",
    "            score = (max_error - threshold) / denominator\n",
    "\n",
    "            # 절대 인덱스를 사용하여 이상을 추가.\n",
    "            anomalies.append([row[0] + window_start, row[1] + window_start, score])\n",
    "\n",
    "        # 이상 목록을 반환.\n",
    "        return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bace8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [1, 5, 7, 8]\n",
    "epsilon = [3, 4, 5, 6]\n",
    "errors > epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa9df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
